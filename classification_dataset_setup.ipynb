{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook provides the functions for running the SGD-to-NTK hybrid strategy model for classification tasks\n",
    "\n",
    "Import necessary libraries. The code can run on gpu, however, with large datasets it won't be possible to store large kernel matrices. If the gpu is not found the program falls back on cpu."
   ],
   "id": "9ce03da326586d5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.lib\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit\n",
    "from jax.nn import initializers\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import time\n",
    "import copy\n",
    "from collections import deque\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# --- JAX Environment Check ---\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"jaxlib version: {jax.lib.__version__}\")\n",
    "backend = jax.default_backend()\n",
    "print(f\"Default backend: {backend}\")\n",
    "print(f\"Available devices: {jax.devices()}\")\n",
    "\n",
    "if backend == 'cpu':\n",
    "    print(\"\\n*** WARNING: JAX is running on CPU. ***\")\n",
    "    print(\"To enable GPU, ensure a CUDA-enabled version of jaxlib is installed.\")\n",
    "    print(\"Example: pip install --upgrade \\\"jax[cuda12_pip]\\\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "The functions for training Neural Network. JAX architecture helps operating with networks parameters and processes more efficiently."
   ],
   "id": "4ecaa7d538a3c267"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def init_network_params(layer_dims, key):      #Initialize network parameters\n",
    "    keys = jax.random.split(key, len(layer_dims) - 1)\n",
    "    params = []\n",
    "    for i, (in_dim, out_dim) in enumerate(zip(layer_dims[:-1], layer_dims[1:])):\n",
    "        W = initializers.glorot_normal()(keys[i], (in_dim, out_dim))\n",
    "        b = initializers.zeros(keys[i], (out_dim,))\n",
    "        params.append((W, b))\n",
    "    return params\n",
    "\n",
    "@jit\n",
    "def jax_relu(x):                             #ReLU activation function\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def jax_softmax(x, axis=-1):                 #Softmax function\n",
    "    x_max = jnp.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = jnp.exp(x - x_max)\n",
    "    return exp_x / jnp.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def jax_forward(params, x):                   #Forward pass of the parameters\n",
    "    activation = x\n",
    "    for i, (W, b) in enumerate(params[:-1]):\n",
    "        outputs = jnp.dot(activation, W) + b\n",
    "        activation = jax_relu(outputs)\n",
    "    final_W, final_b = params[-1]\n",
    "    logits = jnp.dot(activation, final_W) + final_b\n",
    "    return logits\n",
    "\n",
    "\n",
    "def jax_update_params(params, grads, lr):     #Update parameters\n",
    "    return [(W - lr * dW, b - lr * db) for (W, b), (dW, db) in zip(params, grads)]\n",
    "\n",
    "@jit\n",
    "def jax_compute_crossentropy_loss(params, x_batch, y_batch_one_hot):      #Compute cross-entropy loss\n",
    "    \"\"\"Computes Cross-Entropy loss for classification.\"\"\"\n",
    "    logits = jax_forward(params, x_batch)\n",
    "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
    "    return -jnp.sum(y_batch_one_hot.astype(jnp.float32) * log_probs) / x_batch.shape[0]\n",
    "\n",
    "jax_loss_grad_fn = jit(grad(jax_compute_crossentropy_loss, argnums=0))\n",
    "\n",
    "def jax_predict_proba(params, x):             #Activate softmax\n",
    "    return jax_softmax(jax_forward(params, x))\n",
    "\n",
    "def compute_accuracy_jax(params, x, y_one_hot):          #Compute accuracy\n",
    "    \"\"\"Computes accuracy for classification.\"\"\"\n",
    "    preds_proba = jax_predict_proba(params, x)\n",
    "    return jnp.mean(jnp.argmax(preds_proba, axis=1) == jnp.argmax(y_one_hot, axis=1))"
   ],
   "id": "b3b0e8f2f9f386df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Flatten and unflatten parameters to use JAX functions:"
   ],
   "id": "93fcc2ab1c755119"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def flatten_params(params_list):\n",
    "    flat_params_leaves, treedef = jax.tree_util.tree_flatten(params_list)\n",
    "    flat_params_leaves = [jnp.asarray(leaf) for leaf in flat_params_leaves]\n",
    "    return jnp.concatenate([p.ravel() for p in flat_params_leaves]), treedef\n",
    "\n",
    "def unflatten_params(flat_params_vec, treedef, shapes_and_dtypes_meta):\n",
    "    leaves = []\n",
    "    current_pos = 0\n",
    "    for shape, dtype in shapes_and_dtypes_meta:\n",
    "        num_elements = np.prod(shape, dtype=int)\n",
    "        leaves.append(jnp.asarray(flat_params_vec[current_pos: current_pos + num_elements], dtype=dtype).reshape(shape))\n",
    "        current_pos += num_elements\n",
    "    return jax.tree_util.tree_unflatten(treedef, leaves)\n",
    "\n",
    "def get_shapes_and_dtypes(params_list):\n",
    "    flat_params_meta, _ = jax.tree_util.tree_flatten(params_list)\n",
    "    return [(p.shape, p.dtype) for p in flat_params_meta]\n",
    "\n",
    "def single_sample_forward_flat_params(flat_params_vec, single_x_input, treedef, shapes_and_dtypes_meta):\n",
    "    unflattened_params_list = unflatten_params(flat_params_vec, treedef, shapes_and_dtypes_meta)\n",
    "    return jax_forward(unflattened_params_list, single_x_input.reshape(1, -1))[0]"
   ],
   "id": "e4a4754b439acda6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Computes the Frobenius norm of the difference between two parameter lists."
   ],
   "id": "dffb20feb9f353a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_params_diff_norm(params1, params2):\n",
    "    diff_norms_sq = [\n",
    "        jnp.sum((w1 - w2)**2) + jnp.sum((b1 - b2)**2)\n",
    "        for (w1, b1), (w2, b2) in zip(params1, params2)\n",
    "    ]\n",
    "    return jnp.sqrt(jnp.sum(jnp.array(diff_norms_sq)))"
   ],
   "id": "ef63f38b7dedeba4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "One_hot encode the output:"
   ],
   "id": "66cd7c767e97db81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def one_hot(y, num_classes):\n",
    "    y_int = np.asarray(y, dtype=int)\n",
    "    return np.eye(num_classes)[y_int.reshape(-1)]"
   ],
   "id": "64b2fc80f3855090"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Computes the empirical NTK for a multi-class output."
   ],
   "id": "883491eebb277d2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@partial(jit, static_argnames=['num_classes_for_k0'])\n",
    "def compute_empirical_ntk_k0(J_all_at_theta0, num_classes_for_k0):\n",
    "    K0 = jnp.einsum('acp,bcp->ab', J_all_at_theta0, J_all_at_theta0)\n",
    "    return K0"
   ],
   "id": "f16ff5571d71196a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Run SGD training for a number of epochs:"
   ],
   "id": "1d27a53998ef6788"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_sgd_epochs(params_initial, X_train_sgd, Y_train_onehot_sgd, X_val_full, Y_val_onehot_full,\n",
    "                   start_epoch_idx, num_epochs_to_run, # start_epoch_idx is 0-based\n",
    "                   batch_size, lr_sgd, key_sgd_loop, phase_label=\"SGD\"):\n",
    "    print(f\"\\n--- Starting {phase_label} Training Phase (Epochs {start_epoch_idx + 1} to {start_epoch_idx + num_epochs_to_run}) ---\")\n",
    "    params = params_initial # Start from provided parameters\n",
    "    N_train_sgd = X_train_sgd.shape[0]\n",
    "\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    sgd_phase_start_time = time.time()\n",
    "    current_batch_size = min(batch_size, N_train_sgd)\n",
    "\n",
    "    for epoch_offset in range(num_epochs_to_run):\n",
    "        actual_epoch_num_display = start_epoch_idx + epoch_offset + 1 # For printing (1-based)\n",
    "\n",
    "        key_sgd_loop, subkey_perm = jax.random.split(key_sgd_loop)\n",
    "        indices = jax.random.permutation(subkey_perm, N_train_sgd)\n",
    "\n",
    "        # --- Mini-batch update loop ---\n",
    "        for i in range(0, N_train_sgd, current_batch_size):\n",
    "            X_batch = X_train_sgd[indices[i:i + current_batch_size]]\n",
    "            Y_batch = Y_train_onehot_sgd[indices[i:i + current_batch_size]]\n",
    "            grads = jax_loss_grad_fn(params, X_batch, Y_batch)\n",
    "            params = jax_update_params(params, grads, lr_sgd)\n",
    "\n",
    "        # --- Full-dataset metric logging (end of epoch) ---\n",
    "        train_loss = float(jax_compute_crossentropy_loss(params, X_train_sgd, Y_train_onehot_sgd))\n",
    "        train_acc = float(compute_accuracy_jax(params, X_train_sgd, Y_train_onehot_sgd))\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        val_loss = float(jax_compute_crossentropy_loss(params, X_val_full, Y_val_onehot_full))\n",
    "        val_acc = float(compute_accuracy_jax(params, X_val_full, Y_val_onehot_full))\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"{phase_label} Epoch {actual_epoch_num_display} - Train L: {train_loss:.4f}, A: {train_acc*100:.2f}% | Val L: {val_loss:.4f}, A: {val_acc*100:.2f}%\")\n",
    "\n",
    "    sgd_phase_time = time.time() - sgd_phase_start_time\n",
    "    print(f\"{phase_label} phase ({num_epochs_to_run} epochs) took {sgd_phase_time:.2f} seconds.\")\n",
    "    return params, history, sgd_phase_time"
   ],
   "id": "118073148fc44602"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Monitor the switching condition while running SGD training. Finish and return the parameters when condition is met."
   ],
   "id": "6343216fb27a790"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_sgd_monitoring_switch(\n",
    "    params_initial, X_train_sgd, Y_train_onehot_sgd, X_val_full, Y_val_onehot_full,\n",
    "    max_sgd_epochs, batch_size, lr_sgd, key_sgd_loop,\n",
    "    switch_config,\n",
    "    X_ntk_monitor_subset, num_classes\n",
    "):\n",
    "    print(f\"\\n--- Starting SGD Phase (Monitoring for Switch using '{switch_config['method']}') ---\")\n",
    "    params = params_initial\n",
    "    N_train_sgd = X_train_sgd.shape[0]\n",
    "\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    sgd_phase_start_time = time.time()\n",
    "    current_batch_size = min(batch_size, N_train_sgd)\n",
    "\n",
    "    # --- Switch condition specific setup ---\n",
    "    method = switch_config['method']\n",
    "    if method == 'param_norm':\n",
    "        k = switch_config.get('param_norm_window', 3)\n",
    "        param_history = deque(maxlen=k + 1)\n",
    "        param_history.append(copy.deepcopy(params))\n",
    "\n",
    "    elif method == 'ntk_norm':\n",
    "        k = switch_config.get('ntk_norm_window', 3)\n",
    "        ntk_total_diff_history = deque(maxlen=k + 1)\n",
    "\n",
    "        _, treedef = flatten_params(params)\n",
    "        shapes_meta = get_shapes_and_dtypes(params)\n",
    "        partial_apply_fn = partial(single_sample_forward_flat_params, treedef=treedef, shapes_and_dtypes_meta=shapes_meta)\n",
    "        jac_fn_single = jax.jacrev(partial_apply_fn, argnums=0)\n",
    "        J_vmap_fn = jit(jax.vmap(lambda p, x: jac_fn_single(p, x), in_axes=(None, 0), out_axes=0))\n",
    "        params_flat_initial, _ = flatten_params(params_initial)\n",
    "        J_initial = J_vmap_fn(params_flat_initial, X_ntk_monitor_subset)\n",
    "        K_initial = compute_empirical_ntk_k0(J_initial, num_classes)\n",
    "        ntk_total_diff_history.append(0.0)\n",
    "\n",
    "    epoch_at_switch = max_sgd_epochs\n",
    "\n",
    "    for epoch in range(max_sgd_epochs):\n",
    "        actual_epoch_num_display = epoch + 1\n",
    "        epoch_start_time = time.time()\n",
    "        key_sgd_loop, subkey_perm = jax.random.split(key_sgd_loop)\n",
    "        indices = jax.random.permutation(subkey_perm, N_train_sgd)\n",
    "\n",
    "        for i in range(0, N_train_sgd, current_batch_size):\n",
    "            X_batch = X_train_sgd[indices[i:i + current_batch_size]]\n",
    "            Y_batch = Y_train_onehot_sgd[indices[i:i + current_batch_size]]\n",
    "            grads = jax_loss_grad_fn(params, X_batch, Y_batch)\n",
    "            params = jax_update_params(params, grads, lr_sgd)\n",
    "\n",
    "        # Log metrics\n",
    "        train_loss = float(jax_compute_crossentropy_loss(params, X_train_sgd, Y_train_onehot_sgd))\n",
    "        train_acc = float(compute_accuracy_jax(params, X_train_sgd, Y_train_onehot_sgd))\n",
    "        val_loss = float(jax_compute_crossentropy_loss(params, X_val_full, Y_val_onehot_full))\n",
    "        val_acc = float(compute_accuracy_jax(params, X_val_full, Y_val_onehot_full))\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"SGD (Monitoring) Epoch {actual_epoch_num_display} - Train L: {train_loss:.4f}, A: {train_acc*100:.2f}% | Val L: {val_loss:.4f}, A: {val_acc*100:.2f}%\", end='')\n",
    "\n",
    "        # --- Check switch condition ---\n",
    "        switch_now = False\n",
    "\n",
    "        if method == 'fixed_epoch':\n",
    "            if actual_epoch_num_display >= switch_config['fixed_switch_epoch']:\n",
    "                switch_now = True\n",
    "        elif method == 'param_norm':\n",
    "            k = switch_config.get('param_norm_window', 3)\n",
    "            param_history.append(copy.deepcopy(params))\n",
    "            if len(param_history) == k + 1:\n",
    "                diff_norm = compute_params_diff_norm(param_history[-1], param_history[0])\n",
    "                print(f\" | Param diff norm (k={k}) = {diff_norm:.6f}\", end='')\n",
    "                if diff_norm < switch_config['param_norm_threshold']:\n",
    "                    switch_now = True\n",
    "\n",
    "        elif method == 'ntk_norm':\n",
    "            k = switch_config.get('ntk_norm_window', 3)\n",
    "            params_flat_current, _ = flatten_params(params)\n",
    "            J_current = J_vmap_fn(params_flat_current, X_ntk_monitor_subset)\n",
    "            K_current = compute_empirical_ntk_k0(J_current, num_classes)\n",
    "            ntk_diff_total = float(jnp.linalg.norm(K_current - K_initial, 'fro'))\n",
    "            ntk_total_diff_history.append(ntk_diff_total)\n",
    "\n",
    "            if len(ntk_total_diff_history) == k + 1:\n",
    "                diff_norm = abs(ntk_total_diff_history[-1] - ntk_total_diff_history[0])\n",
    "                print(f\" | NTK stability (k={k}) = {diff_norm:.4f}\", end='')\n",
    "                if diff_norm < switch_config['ntk_norm_threshold']:\n",
    "                    switch_now = True\n",
    "\n",
    "        print(f\" (took {time.time() - epoch_start_time:.2f}s)\")\n",
    "        if switch_now:\n",
    "            epoch_at_switch = actual_epoch_num_display\n",
    "            print(f\">>> Switching condition '{method}' met at epoch {epoch_at_switch} over a {k}-epoch window. <<<\")\n",
    "            break\n",
    "\n",
    "    if epoch == max_sgd_epochs - 1 and not switch_now:\n",
    "        epoch_at_switch = max_sgd_epochs\n",
    "        print(f\">>> Max SGD epochs ({max_sgd_epochs}) reached without meeting switch condition. This run will be flagged. <<<\")\n",
    "\n",
    "    sgd_phase_time = time.time() - sgd_phase_start_time\n",
    "    print(f\"SGD monitoring phase ({epoch_at_switch} epochs) took {sgd_phase_time:.2f} seconds.\")\n",
    "\n",
    "    final_history = {k: v[:epoch_at_switch] for k, v in history.items()}\n",
    "    return params, final_history, sgd_phase_time, epoch_at_switch"
   ],
   "id": "8cd5d5c4587abfac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Run a full SGD training to find the switching point:"
   ],
   "id": "719f2560ba15e471"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_sgd_scouting(\n",
    "    params_initial, X_train_sgd, Y_train_onehot_sgd, X_val_full, Y_val_onehot_full,\n",
    "    scouting_epochs, batch_size, lr_sgd, key_sgd_loop,\n",
    "    switch_method, # 'param_norm' or 'ntk_norm'\n",
    "    X_ntk_monitor_subset, num_classes,\n",
    "    param_norm_window, ntk_norm_window\n",
    "):\n",
    "    print(f\"\\n--- Starting SGD Scouting Run for {scouting_epochs} Epochs (Method: {switch_method}) ---\")\n",
    "    params = params_initial\n",
    "    N_train_sgd = X_train_sgd.shape[0]\n",
    "    history = {'val_loss': [], 'val_acc': [], 'norm_diff': []}\n",
    "    scouting_start_time = time.time()\n",
    "    current_batch_size = min(batch_size, N_train_sgd)\n",
    "\n",
    "    # --- Monitoring setup based on the chosen method ---\n",
    "    if switch_method == 'param_norm':\n",
    "        k = param_norm_window\n",
    "        param_history = deque(maxlen=k + 1)\n",
    "        param_history.append(copy.deepcopy(params))\n",
    "\n",
    "    elif switch_method == 'ntk_norm':\n",
    "        k = ntk_norm_window\n",
    "        ntk_total_diff_history = deque(maxlen=k + 1)\n",
    "\n",
    "        _, treedef = flatten_params(params)\n",
    "        shapes_meta = get_shapes_and_dtypes(params)\n",
    "        partial_apply_fn = partial(single_sample_forward_flat_params, treedef=treedef, shapes_and_dtypes_meta=shapes_meta)\n",
    "        jac_fn_single = jax.jacrev(partial_apply_fn, argnums=0)\n",
    "        J_vmap_fn = jit(jax.vmap(lambda p, x: jac_fn_single(p, x), in_axes=(None, 0), out_axes=0))\n",
    "        params_flat_initial, _ = flatten_params(params_initial)\n",
    "        J_initial = J_vmap_fn(params_flat_initial, X_ntk_monitor_subset)\n",
    "        K_initial = compute_empirical_ntk_k0(J_initial, num_classes)\n",
    "        ntk_total_diff_history.append(0.0)\n",
    "\n",
    "    for epoch in range(scouting_epochs):\n",
    "        key_sgd_loop, subkey_perm = jax.random.split(key_sgd_loop)\n",
    "        indices = jax.random.permutation(subkey_perm, N_train_sgd)\n",
    "\n",
    "        for i in range(0, N_train_sgd, current_batch_size):\n",
    "            X_batch = X_train_sgd[indices[i:i + current_batch_size]]\n",
    "            Y_batch = Y_train_onehot_sgd[indices[i:i + current_batch_size]]\n",
    "            grads = jax_loss_grad_fn(params, X_batch, Y_batch)\n",
    "            params = jax_update_params(params, grads, lr_sgd)\n",
    "\n",
    "        # Log performance metrics\n",
    "        history['val_loss'].append(float(jax_compute_crossentropy_loss(params, X_val_full, Y_val_onehot_full)))\n",
    "        val_acc = float(compute_accuracy_jax(params, X_val_full, Y_val_onehot_full))\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        norm_diff = np.nan\n",
    "        # --- Unified calculation logic ---\n",
    "        if switch_method == 'param_norm':\n",
    "            k = param_norm_window\n",
    "            param_history.append(copy.deepcopy(params))\n",
    "            if len(param_history) == k + 1:\n",
    "                 norm_diff = compute_params_diff_norm(param_history[-1], param_history[0])\n",
    "\n",
    "        elif switch_method == 'ntk_norm':\n",
    "            k = ntk_norm_window\n",
    "            params_flat_current, _ = flatten_params(params)\n",
    "            J_current = J_vmap_fn(params_flat_current, X_ntk_monitor_subset)\n",
    "            K_current = compute_empirical_ntk_k0(J_current, num_classes)\n",
    "            ntk_diff_total = float(jnp.linalg.norm(K_current - K_initial, 'fro'))\n",
    "\n",
    "            ntk_total_diff_history.append(ntk_diff_total)\n",
    "            if len(ntk_total_diff_history) == k + 1:\n",
    "                norm_diff = abs(ntk_total_diff_history[-1] - ntk_total_diff_history[0])\n",
    "\n",
    "        history['norm_diff'].append(float(norm_diff))\n",
    "\n",
    "        print(f\"Scouting Epoch {epoch + 1}/{scouting_epochs} - Val Acc: {val_acc*100:.2f}% | Norm Diff (k={k}): {norm_diff:.4f}\")\n",
    "\n",
    "    print(f\"Scouting run took {time.time() - scouting_start_time:.2f} seconds.\")\n",
    "    return history"
   ],
   "id": "997aca878bc2f970"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Run NTK 1 phase:"
   ],
   "id": "b1363ca9e803fabd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_ntk1_phase(params_sgd, X_train_ntk, Y_train_onehot_ntk, X_val_full, Y_val_onehot_full,\n",
    "                   ntk_epochs, batch_size, lr_ntk, key_ntk_loop):\n",
    "    print(\"\\n--- Starting NTK 1 Phase ---\")\n",
    "    N_train_ntk = X_train_ntk.shape[0]\n",
    "\n",
    "    theta_0_params_unflat = params_sgd\n",
    "    theta_0_flat, treedef_0 = flatten_params(theta_0_params_unflat)\n",
    "    shapes_meta_0 = get_shapes_and_dtypes(theta_0_params_unflat)\n",
    "    theta_k_flat = jnp.copy(theta_0_flat)\n",
    "\n",
    "    partial_apply_fn = partial(single_sample_forward_flat_params, treedef=treedef_0, shapes_and_dtypes_meta=shapes_meta_0)\n",
    "    jac_fn_single_sample = jax.jacrev(partial_apply_fn, argnums=0)\n",
    "    J_at_theta0_single_sample = jit(lambda single_x: jac_fn_single_sample(theta_0_flat, single_x))\n",
    "    J_batch_at_theta0_vmap = jit(jax.vmap(J_at_theta0_single_sample, in_axes=(0), out_axes=0))\n",
    "    predict_batch_theta_k_vmap = jit(\n",
    "        jax.vmap(partial(single_sample_forward_flat_params, treedef=treedef_0, shapes_and_dtypes_meta=shapes_meta_0),\n",
    "                 in_axes=(None, 0), out_axes=0))\n",
    "\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    ntk1_start_time = time.time()\n",
    "\n",
    "    for ntk_iter in range(ntk_epochs):\n",
    "        key_ntk_loop, subkey_perm = jax.random.split(key_ntk_loop)\n",
    "        indices = jax.random.permutation(subkey_perm, N_train_ntk)\n",
    "        total_param_update_contrib = jnp.zeros_like(theta_k_flat)\n",
    "        for i in range(0, N_train_ntk, batch_size):\n",
    "            X_batch = X_train_ntk[indices[i:i + batch_size]]\n",
    "            Y_batch_onehot = Y_train_onehot_ntk[indices[i:i + batch_size]]\n",
    "            J_b_at_theta0 = J_batch_at_theta0_vmap(X_batch)\n",
    "            logits_b_at_thetak = predict_batch_theta_k_vmap(theta_k_flat, X_batch)\n",
    "            pred_probas_b_at_thetak = jax_softmax(logits_b_at_thetak)\n",
    "            Error_batch = pred_probas_b_at_thetak - Y_batch_onehot\n",
    "            batch_contrib = jnp.einsum('bcp,bc->p', J_b_at_theta0, Error_batch)\n",
    "            total_param_update_contrib += batch_contrib\n",
    "\n",
    "        effective_lr_ntk = (2.0 * lr_ntk) / N_train_ntk\n",
    "        theta_k_flat -= effective_lr_ntk * total_param_update_contrib\n",
    "\n",
    "        current_params_unflat = unflatten_params(theta_k_flat, treedef_0, shapes_meta_0)\n",
    "        train_loss = float(jax_compute_crossentropy_loss(current_params_unflat, X_train_ntk, Y_train_onehot_ntk))\n",
    "        train_acc = float(compute_accuracy_jax(current_params_unflat, X_train_ntk, Y_train_onehot_ntk))\n",
    "        val_loss = float(jax_compute_crossentropy_loss(current_params_unflat, X_val_full, Y_val_onehot_full))\n",
    "        val_acc = float(compute_accuracy_jax(current_params_unflat, X_val_full, Y_val_onehot_full))\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        print(f\"NTK 1 Iter {ntk_iter + 1}/{ntk_epochs} - Train (full) L: {train_loss:.4f}, A: {train_acc*100:.2f}% | Val (full) L: {val_loss:.4f}, A: {val_acc*100:.2f}%\")\n",
    "\n",
    "    ntk1_time = time.time() - ntk1_start_time\n",
    "    print(f\"NTK 1 phase ({ntk_epochs} iterations) took {ntk1_time:.2f} seconds.\")\n",
    "    final_params_ntk1 = unflatten_params(theta_k_flat, treedef_0, shapes_meta_0)\n",
    "    return final_params_ntk1, history, ntk1_time"
   ],
   "id": "1cf7ab828c45066c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Two functions for computing matrix exponential, either using Taylor epxansion or a direct calculation:"
   ],
   "id": "719fcebe11e2e9aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def matrix_exp_taylor(A, order=5):\n",
    "    \"\"\"Taylor expansion for matrix exponential: I + A + A^2/2! + ...\"\"\"\n",
    "    N = A.shape[0]\n",
    "    if A.shape[0] != A.shape[1]:\n",
    "        raise ValueError(\"Matrix must be square for exponential.\")\n",
    "\n",
    "    result = jnp.eye(N, dtype=A.dtype)\n",
    "    A_power_k = jnp.eye(N, dtype=A.dtype)\n",
    "    factorial_k = 1.0\n",
    "\n",
    "    for k in range(1, order + 1):\n",
    "        A_power_k = jnp.dot(A_power_k, A)\n",
    "        factorial_k *= k\n",
    "        result += A_power_k / factorial_k\n",
    "    return result\n",
    "\n",
    "USE_JAX_EXPM = True # Set to False to use Taylor expansion\n",
    "\n",
    "def compute_matrix_exp(A, taylor_order=5):\n",
    "    if USE_JAX_EXPM:\n",
    "        try:\n",
    "            return jax.scipy.linalg.expm(A)\n",
    "        except Exception as e:\n",
    "            print(f\"jax.scipy.linalg.expm failed: {e}. Falling back to Taylor expansion.\")\n",
    "            return matrix_exp_taylor(A, order=taylor_order)\n",
    "    else:\n",
    "        return matrix_exp_taylor(A, order=taylor_order)"
   ],
   "id": "7383556aaa0b2506"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

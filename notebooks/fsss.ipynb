{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "from hybrid_ntk import data_utils, utils, training"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fetch data:",
   "id": "8487625f496473a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train_full, X_val_full, X_test_full, Y_train_onehot_full, Y_val_onehot_full, Y_test_onehot_full = fetch_split_data('Fashion-MNIST')",
   "id": "4287004f6dc4e8c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create random keys:",
   "id": "e37e67038cf81396"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "scout_key_loop, run_keys = randomize(num_seeds = 10)",
   "id": "b45300e7b80b65b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initialize network:",
   "id": "6ef8af95e408d91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "layer_dims = [X_train_full.shape[1], 100, 100, num_classes]  # Network size\n",
    "print(f\"Network layer_dims: {layer_dims}\")\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "\n",
    "# Scouting Run Config\n",
    "scouting_epochs = 15\n",
    "scouting_lr = 0.007\n",
    "scouting_method = 'param_norm' # 'param_norm' or 'ntk_norm'\n",
    "\n",
    "# Main Experiment Config\n",
    "epochs_sgd_total_config = 15\n",
    "batch_size_config = 128\n",
    "\n",
    "# Switching condition configuration\n",
    "switch_config = {\n",
    "    'method': scouting_method,\n",
    "    'fixed_switch_epoch': 7,\n",
    "    'param_norm_window': 2,    # Window size k\n",
    "    'param_norm_threshold': 0.3253,\n",
    "    'ntk_norm_window': 4,      # Window size k\n",
    "    'ntk_norm_threshold': 253892,\n",
    "}\n",
    "\n",
    "lr_sgd_config = 0.007\n",
    "lr_ntk_iterative_config = 0.007\n",
    "lambda_ntk3_config = 0.01\n",
    "taylor_order_config = 3\n",
    "\n",
    "dataset_name = \"Fashion-MNIST\"\n",
    "\n",
    "# Max samples for NTK computation phases.\n",
    "max_ntk_samples_val = 2000\n",
    "max_ntk_scouting_val = 200\n",
    "\n",
    "print(\n",
    "    f\"SGD will run on {X_train_full.shape[0]} samples. NTK phases will use a subset of {max_ntk_samples_val} samples.\")"
   ],
   "id": "4157af780b1787cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scouting run:",
   "id": "1f880980c7951322"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Process AVERAGED Data for Threshold Calculation ---\n",
    "avg_val_loss = np.array(avg_scout_history['val_loss']['mean'])\n",
    "avg_norm_diff = np.array(avg_scout_history['norm_diff']['mean'])\n",
    "\n",
    "# --- Process SINGLE-RUN Data for Plotting ---\n",
    "epochs = np.arange(1, scouting_epochs + 1)\n",
    "val_loss = np.array(first_run_history['val_loss'])\n",
    "val_acc = np.array(first_run_history['val_acc'])\n",
    "norm_diff = np.array(first_run_history['norm_diff'])\n",
    "\n",
    "# --- ROBUST THRESHOLD SUGGESTION FROM AVERAGED DATA ---\n",
    "print(\"\\n--- Suggested Thresholds Based on Averaged Scouting Run ---\")\n",
    "val_loss_diff = np.diff(avg_val_loss)\n",
    "try:\n",
    "    initial_drop = avg_val_loss[0] - np.min(avg_val_loss)\n",
    "    elbow_epoch_idx = np.where(np.abs(val_loss_diff) < 0.01 * initial_drop)[0][0] + 1\n",
    "except (IndexError, TypeError):\n",
    "    elbow_epoch_idx = len(avg_val_loss) // 2\n",
    "print(f\"Average validation loss stabilized around epoch {elbow_epoch_idx + 1}.\")\n",
    "\n",
    "stabilization_norms = avg_norm_diff[elbow_epoch_idx:]\n",
    "stabilization_norms = stabilization_norms[~np.isnan(stabilization_norms)]\n",
    "\n",
    "recommended_threshold = 0.0\n",
    "suggested_switch_epoch = scouting_epochs\n",
    "\n",
    "if stabilization_norms.size > 1:\n",
    "    if scouting_method == 'param_norm':\n",
    "        print(\"\\nUsing 'Early Switch' (75th percentile) heuristic for 'param_norm'.\")\n",
    "        recommended_threshold = np.percentile(stabilization_norms, 75)\n",
    "        switch_config['param_norm_threshold'] = recommended_threshold\n",
    "    elif scouting_method == 'ntk_norm':\n",
    "        print(\"\\nUsing 'Early Switch' (75th percentile) heuristic for 'ntk_norm'.\")\n",
    "        recommended_threshold = np.percentile(stabilization_norms, 75)\n",
    "        switch_config['ntk_norm_threshold'] = recommended_threshold\n",
    "\n",
    "    try:\n",
    "        suggested_switch_epoch = np.where(avg_norm_diff < recommended_threshold)[0][0] + 1\n",
    "    except IndexError:\n",
    "        print(\"Threshold was not met on averaged data. Defaulting to last epoch.\")\n",
    "        suggested_switch_epoch = scouting_epochs\n",
    "else:\n",
    "    print(\"Could not automatically determine a threshold.\")\n",
    "\n",
    "print(f\"\\nRecommended Threshold for '{scouting_method}': {recommended_threshold:.4f}\")\n",
    "print(f\"This threshold suggests a switch at Epoch: {suggested_switch_epoch}\")"
   ],
   "id": "a432e8c67377dff5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run scouting:",
   "id": "b75cb1465dbb2132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- DYNAMIC FILENAME GENERATION ---\n",
    "dataset_prefix = f\"{dataset_name.lower().replace('-', '')}_sc_avg\"\n",
    "network_width = layer_dims[1]\n",
    "metric_char = 'n' if scouting_method == 'ntk_norm' else 'p'\n",
    "filename_template = f\"{dataset_prefix}_{{metric_type}}_{metric_char}_{network_width}_{scouting_lr}_{recommended_threshold:.4f}.png\"\n",
    "\n",
    "# --- PLOTTING SETUP ---\n",
    "# Define font sizes for better readability in a report\n",
    "TITLE_FONT = 18\n",
    "LABEL_FONT = 24\n",
    "LEGEND_FONT = 22\n",
    "TICK_FONT = 12\n",
    "\n",
    "# --- PLOTTING (Using data from the FIRST run for visualization) ---\n",
    "switch_idx = min(suggested_switch_epoch - 1, len(val_loss) - 1)\n",
    "loss_at_switch = val_loss[switch_idx]\n",
    "acc_at_switch = val_acc[switch_idx] * 100\n",
    "\n",
    "if scouting_method == 'param_norm':\n",
    "    metric_label, linestyle, marker, color = r'Parameter Norm Difference ($\\epsilon$)', '--', 'x', 'tab:blue'\n",
    "    window_val, title_suffix = switch_config['param_norm_window'], 'Parameter Stability'\n",
    "    legend_label = fr'$||\\theta_t - \\theta_{{t-{window_val}}}||_F$'\n",
    "else: # ntk_norm\n",
    "    metric_label, linestyle, marker, color = r'NTK Stability ($\\delta$)', ':', 's', 'tab:green'\n",
    "    window_val, title_suffix = switch_config['ntk_norm_window'], 'NTK Stability'\n",
    "    legend_label = fr'| $||\\Theta_t - \\Theta_0||_F - ||\\Theta_{{t-{window_val}}}-\\Theta_0||_F$ |'\n",
    "\n",
    "# PLOT 1: Validation Loss vs. Stability\n",
    "fig1, ax1 = plt.subplots(1, 1, figsize=(12, 7))\n",
    "ax1.plot(epochs, val_loss, color='tab:red', marker='o', label='Validation Loss (Run 1)', zorder=5)\n",
    "ax1.plot(suggested_switch_epoch, loss_at_switch, '*', color='magenta', markersize=20, label=f'Suggested Switch (Ep. {suggested_switch_epoch})', zorder=10, markeredgecolor='black')\n",
    "ax1.set_ylabel('Validation Loss', color='tab:red', fontsize=LABEL_FONT)\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red', labelsize=TICK_FONT)\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(epochs, norm_diff, color=color, marker=marker, linestyle=linestyle, label=legend_label)\n",
    "ax1_twin.set_ylabel(metric_label, color=color, fontsize=LABEL_FONT)\n",
    "ax1_twin.tick_params(axis='y', labelcolor=color, labelsize=TICK_FONT)\n",
    "ax1.set_xlabel('Epoch', fontsize=LABEL_FONT)\n",
    "ax1.tick_params(axis='x', labelsize=TICK_FONT)\n",
    "ax1.grid(True)\n",
    "fig1.legend(loc=\"upper right\", bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes, fontsize=LEGEND_FONT)\n",
    "fig1.tight_layout()\n",
    "val_filename = filename_template.format(metric_type='val')\n",
    "plt.savefig(val_filename)\n",
    "print(f\"\\nPlot saved as {val_filename}\")\n",
    "plt.show()\n",
    "\n",
    "# PLOT 2: Accuracy vs. Stability\n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(12, 7))\n",
    "ax2.plot(epochs, val_acc * 100, color='tab:purple', marker='o', label='Validation Accuracy (Run 1)', zorder=5)\n",
    "ax2.plot(suggested_switch_epoch, acc_at_switch, '*', color='magenta', markersize=20, label=f'Suggested Switch (Ep. {suggested_switch_epoch})', zorder=10, markeredgecolor='black')\n",
    "ax2.set_ylabel('Validation Accuracy (%)', color='tab:purple', fontsize=LABEL_FONT)\n",
    "ax2.tick_params(axis='y', labelcolor='tab:purple', labelsize=TICK_FONT)\n",
    "ax2_twin = ax2.twinx()\n",
    "ax2_twin.plot(epochs, norm_diff, color=color, marker=marker, linestyle=linestyle, label=legend_label)\n",
    "ax2_twin.set_ylabel(metric_label, color=color, fontsize=LABEL_FONT)\n",
    "ax2_twin.tick_params(axis='y', labelcolor=color, labelsize=TICK_FONT)\n",
    "ax2.set_xlabel('Epoch', fontsize=LABEL_FONT)\n",
    "ax2.tick_params(axis='x', labelsize=TICK_FONT)\n",
    "ax2.grid(True)\n",
    "# --- CORRECTED LINE ---\n",
    "fig2.legend(loc=\"lower left\", bbox_to_anchor=(0.1, 0.1), bbox_transform=ax2.transAxes, fontsize=LEGEND_FONT)\n",
    "fig2.tight_layout()\n",
    "acc_filename = filename_template.format(metric_type='acc')\n",
    "plt.savefig(acc_filename)\n",
    "print(f\"Plot saved as {acc_filename}\")\n",
    "plt.show()"
   ],
   "id": "6449ce7dc09384e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run training:",
   "id": "11b9107d8fd1c8dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_runs_histories = {'SGD_Part1': [], 'SGD_Part2': [], 'NTK1_from_Switch': [], 'NTK2_from_Switch': [], 'NTK3_from_Switch': []}\n",
    "all_runs_times = {'SGD (Part 1)': [], 'SGD (Part 2)': [], 'NTK1': [], 'NTK2': [], 'NTK3': []}\n",
    "all_runs_test_metrics = {'Params_At_Switch': [], 'SGD_Full_Run': [], 'NTK1_from_Switch': [], 'NTK2_from_Switch': [], 'NTK3_from_Switch': []}\n",
    "all_runs_switch_epochs = []\n",
    "\n",
    "for seed in range(num_seeds):\n",
    "    print(f\"\\n\\n===== RUNNING EXPERIMENT FOR SEED {seed + 1}/{num_seeds} =====\")\n",
    "    key_init_params, key_sgd_part1, key_sgd_part2, key_ntk_subset_shuffle, key_ntk1, key_ntk2, key_ntk3 = run_keys[seed]\n",
    "    initial_model_params = init_network_params(layer_dims, key_init_params)\n",
    "\n",
    "    # --- CORRECTED: Create distinct subsets for monitoring and NTK training ---\n",
    "    key_monitor_subset, key_train_subset = jax.random.split(key_ntk_subset_shuffle)\n",
    "\n",
    "    # Subset for NTK stability monitoring during the first SGD phase\n",
    "    X_ntk_monitor_subset = X_train_full\n",
    "    if max_ntk_scouting_val is not None and max_ntk_scouting_val < X_train_full.shape[0]:\n",
    "        monitor_indices = jax.random.choice(key_monitor_subset, X_train_full.shape[0],\n",
    "                                            shape=(max_ntk_scouting_val,), replace=False)\n",
    "        X_ntk_monitor_subset = X_train_full[monitor_indices]\n",
    "\n",
    "    # Subset for the actual NTK training phases (NTK1, NTK2, NTK3)\n",
    "    X_train_ntk_subset, Y_train_onehot_ntk_subset = X_train_full, Y_train_onehot_full\n",
    "    if max_ntk_samples_val is not None and max_ntk_samples_val < X_train_full.shape[0]:\n",
    "        train_indices = jax.random.choice(key_train_subset, X_train_full.shape[0],\n",
    "                                          shape=(max_ntk_samples_val,), replace=False)\n",
    "        X_train_ntk_subset = X_train_full[train_indices]\n",
    "        Y_train_onehot_ntk_subset = Y_train_onehot_full[train_indices]\n",
    "\n",
    "    # --- Initial SGD Phase (up to switch point) ---\n",
    "    params_at_switch, history_sgd_part1, time_sgd_part1, epoch_at_switch = run_sgd_monitoring_switch(\n",
    "        initial_model_params, X_train_full, Y_train_onehot_full, X_val_full, Y_val_onehot_full,\n",
    "        max_sgd_epochs=epochs_sgd_total_config,\n",
    "        batch_size=min(batch_size_config, X_train_full.shape[0]),\n",
    "        lr_sgd=lr_sgd_config, key_sgd_loop=key_sgd_part1,\n",
    "        switch_config=switch_config,\n",
    "        X_ntk_monitor_subset=X_ntk_monitor_subset, num_classes=num_classes\n",
    "    )\n",
    "    all_runs_switch_epochs.append(epoch_at_switch)\n",
    "    all_runs_histories['SGD_Part1'].append(history_sgd_part1)\n",
    "    all_runs_times['SGD (Part 1)'].append(time_sgd_part1)\n",
    "    test_l_s, test_a_s = evaluate_on_test_jax(params_at_switch, X_test_full, Y_test_onehot_full)\n",
    "    all_runs_test_metrics['Params_At_Switch'].append({'loss': test_l_s, 'acc': test_a_s})\n",
    "\n",
    "    if epoch_at_switch >= epochs_sgd_total_config:\n",
    "        print(f\"WARNING: Switch condition not met for seed {seed+1}. Adjust thresholds or total epochs.\")\n",
    "        all_runs_histories['SGD_Part2'].append({'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []})\n",
    "        all_runs_times['SGD (Part 2)'].append(0.0)\n",
    "        all_runs_test_metrics['SGD_Full_Run'].append({'loss': test_l_s, 'acc': test_a_s})\n",
    "        all_runs_test_metrics['NTK1_from_Switch'].append({'loss': np.nan, 'acc': np.nan})\n",
    "        all_runs_test_metrics['NTK2_from_Switch'].append({'loss': np.nan, 'acc': np.nan})\n",
    "        all_runs_test_metrics['NTK3_from_Switch'].append({'loss': np.nan, 'acc': np.nan})\n",
    "        continue\n",
    "\n",
    "    num_ntk_iterations_to_run = epochs_sgd_total_config - epoch_at_switch\n",
    "\n",
    "    # --- Continued SGD Phase ---\n",
    "    params_after_full_sgd, history_sgd_part2, time_sgd_part2 = run_sgd_epochs(\n",
    "        copy.deepcopy(params_at_switch), X_train_full, Y_train_onehot_full, X_val_full, Y_val_onehot_full,\n",
    "        start_epoch_idx=epoch_at_switch, num_epochs_to_run=num_ntk_iterations_to_run,\n",
    "        batch_size=min(batch_size_config, X_train_full.shape[0]),\n",
    "        lr_sgd=lr_sgd_config, key_sgd_loop=key_sgd_part2, phase_label=\"SGD (Part 2)\"\n",
    "    )\n",
    "    all_runs_histories['SGD_Part2'].append(history_sgd_part2)\n",
    "    all_runs_times['SGD (Part 2)'].append(time_sgd_part2)\n",
    "    test_l_full, test_a_full = evaluate_on_test_jax(params_after_full_sgd, X_test_full, Y_test_onehot_full)\n",
    "    all_runs_test_metrics['SGD_Full_Run'].append({'loss': test_l_full, 'acc': test_a_full})\n",
    "\n",
    "    # --- NTK Phases ---\n",
    "    current_ntk_batch_size = min(batch_size_config, X_train_ntk_subset.shape[0])\n",
    "    if num_ntk_iterations_to_run > 0:\n",
    "        params_after_ntk1, history_ntk1, time_ntk1 = run_ntk1_phase(\n",
    "            copy.deepcopy(params_at_switch), X_train_ntk_subset, Y_train_onehot_ntk_subset,\n",
    "            X_val_full, Y_val_onehot_full, num_ntk_iterations_to_run,\n",
    "            current_ntk_batch_size, lr_ntk_iterative_config, key_ntk1)\n",
    "        all_runs_histories['NTK1_from_Switch'].append(history_ntk1)\n",
    "        all_runs_times['NTK1'].append(time_ntk1)\n",
    "        test_l_ntk1, test_a_ntk1 = evaluate_on_test_jax(params_after_ntk1, X_test_full, Y_test_onehot_full)\n",
    "        all_runs_test_metrics['NTK1_from_Switch'].append({'loss': test_l_ntk1, 'acc': test_a_ntk1})\n",
    "\n",
    "        params_after_ntk2, history_ntk2, time_ntk2 = run_ntk2_phase(\n",
    "            copy.deepcopy(params_at_switch), X_train_ntk_subset, Y_train_onehot_ntk_subset,\n",
    "            X_val_full, Y_val_onehot_full, epoch_at_switch, num_ntk_iterations_to_run,\n",
    "            lr_ntk_iterative_config, taylor_order_ntk2=taylor_order_config)\n",
    "        all_runs_histories['NTK2_from_Switch'].append(history_ntk2)\n",
    "        all_runs_times['NTK2'].append(time_ntk2)\n",
    "        test_l_ntk2, test_a_ntk2 = evaluate_on_test_jax(params_after_ntk2, X_test_full, Y_test_onehot_full)\n",
    "        all_runs_test_metrics['NTK2_from_Switch'].append({'loss': test_l_ntk2, 'acc': test_a_ntk2})\n",
    "\n",
    "    params_after_ntk3, history_ntk3, time_ntk3 = run_ntk3_phase(\n",
    "        copy.deepcopy(params_at_switch), X_train_ntk_subset, Y_train_onehot_ntk_subset,\n",
    "        X_val_full, Y_val_onehot_full,\n",
    "        lr_ntk=lr_ntk_iterative_config,\n",
    "        lambda_ntk3_reg=lambda_ntk3_config,\n",
    "        T_factor_ntk3=epochs_sgd_total_config,\n",
    "        taylor_order_ntk3=taylor_order_config)\n",
    "    all_runs_histories['NTK3_from_Switch'].append(history_ntk3)\n",
    "    all_runs_times['NTK3'].append(time_ntk3)\n",
    "    test_l_ntk3, test_a_ntk3 = evaluate_on_test_jax(params_after_ntk3, X_test_full, Y_test_onehot_full)\n",
    "    all_runs_test_metrics['NTK3_from_Switch'].append({'loss': test_l_ntk3, 'acc': test_a_ntk3})"
   ],
   "id": "85e3868b70cac48b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Average results:",
   "id": "54aa349e63d195f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "avg_histories = {}\n",
    "avg_histories['SGD_Part1'] = aggregate_histories(all_runs_histories['SGD_Part1'], metric_keys=['val_loss', 'val_acc'])\n",
    "avg_histories['SGD_Part2'] = aggregate_histories(all_runs_histories['SGD_Part2'], metric_keys=['train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "avg_histories['NTK1_from_Switch'] = aggregate_histories(all_runs_histories['NTK1_from_Switch'], metric_keys=['train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "avg_histories['NTK2_from_Switch'] = aggregate_histories(all_runs_histories['NTK2_from_Switch'], metric_keys=['train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "ntk3_final_train_losses = [h['train_loss'][0] for h in all_runs_histories['NTK3_from_Switch'] if h['train_loss']]\n",
    "ntk3_final_val_losses   = [h['val_loss'][0] for h in all_runs_histories['NTK3_from_Switch'] if h['val_loss']]\n",
    "ntk3_final_train_accs = [h['train_acc'][0] for h in all_runs_histories['NTK3_from_Switch'] if h['train_acc']]\n",
    "ntk3_final_val_accs   = [h['val_acc'][0] for h in all_runs_histories['NTK3_from_Switch'] if h['val_acc']]\n",
    "\n",
    "avg_histories['NTK3_from_Switch_Aggregated'] = {\n",
    "    'train_loss': aggregate_scalar_metrics(ntk3_final_train_losses),\n",
    "    'val_loss':   aggregate_scalar_metrics(ntk3_final_val_losses),\n",
    "    'train_acc': aggregate_scalar_metrics(ntk3_final_train_accs),\n",
    "    'val_acc':  aggregate_scalar_metrics(ntk3_final_val_accs)\n",
    "}\n",
    "\n",
    "avg_switch_epoch = np.mean(all_runs_switch_epochs)\n",
    "\n",
    "avg_times = {phase: np.mean(times) for phase, times in all_runs_times.items() if times}\n",
    "avg_test_metrics = {}\n",
    "for phase, metrics_list in all_runs_test_metrics.items():\n",
    "    if metrics_list:\n",
    "        avg_test_metrics[phase] = {\n",
    "            'loss_mean': np.nanmean([m.get('loss', np.nan) for m in metrics_list]),\n",
    "            'loss_std':  np.nanstd([m.get('loss', np.nan) for m in metrics_list]),\n",
    "            'acc_mean': np.nanmean([m.get('acc', np.nan) for m in metrics_list]),\n",
    "            'acc_std':  np.nanstd([m.get('acc', np.nan) for m in metrics_list])\n",
    "        }"
   ],
   "id": "f35f926b4ff60144"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Summarise results:",
   "id": "c95f56831ca8e536"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "avg_histories = {}\n",
    "avg_histories['SGD_Part1'] = aggregate_histories(all_runs_histories['SGD_Part1'], metric_keys=['val_loss', 'val_acc'])\n",
    "avg_histories['SGD_Part2'] = aggregate_histories(all_runs_histories['SGD_Part2'], metric_keys=['train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "avg_histories['NTK1_from_Switch'] = aggregate_histories(all_runs_histories['NTK1_from_Switch'], metric_keys=['train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "avg_histories['NTK2_from_Switch'] = aggregate_histories(all_runs_histories['NTK2_from_Switch'], metric_keys=['train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "ntk3_final_train_losses = [h['train_loss'][0] for h in all_runs_histories['NTK3_from_Switch'] if h['train_loss']]\n",
    "ntk3_final_val_losses   = [h['val_loss'][0] for h in all_runs_histories['NTK3_from_Switch'] if h['val_loss']]\n",
    "ntk3_final_train_accs = [h['train_acc'][0] for h in all_runs_histories['NTK3_from_Switch'] if h['train_acc']]\n",
    "ntk3_final_val_accs   = [h['val_acc'][0] for h in all_runs_histories['NTK3_from_Switch'] if h['val_acc']]\n",
    "\n",
    "avg_histories['NTK3_from_Switch_Aggregated'] = {\n",
    "    'train_loss': aggregate_scalar_metrics(ntk3_final_train_losses),\n",
    "    'val_loss':   aggregate_scalar_metrics(ntk3_final_val_losses),\n",
    "    'train_acc': aggregate_scalar_metrics(ntk3_final_train_accs),\n",
    "    'val_acc':  aggregate_scalar_metrics(ntk3_final_val_accs)\n",
    "}\n",
    "\n",
    "avg_switch_epoch = np.mean(all_runs_switch_epochs)\n",
    "\n",
    "avg_times = {phase: np.mean(times) for phase, times in all_runs_times.items() if times}\n",
    "avg_test_metrics = {}\n",
    "for phase, metrics_list in all_runs_test_metrics.items():\n",
    "    if metrics_list:\n",
    "        avg_test_metrics[phase] = {\n",
    "            'loss_mean': np.nanmean([m.get('loss', np.nan) for m in metrics_list]),\n",
    "            'loss_std':  np.nanstd([m.get('loss', np.nan) for m in metrics_list]),\n",
    "            'acc_mean': np.nanmean([m.get('acc', np.nan) for m in metrics_list]),\n",
    "            'acc_std':  np.nanstd([m.get('acc', np.nan) for m in metrics_list])\n",
    "        }"
   ],
   "id": "67aa199029138674"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
